{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15af201-9ff4-49ca-a7f2-2df7cab6e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: -PKG-VERSION is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: -PKG-VERSION is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "class NeonatalVentilationDataset(pl.LightningDataModule):\n",
    "    def __init__(self, train_index, train_wf, test_index, test_wf, target, batch_size, fixed_len = None, oversample=True, center=True):\n",
    "        self.df = pd.read_csv(train_index)\n",
    "        self.waveforms = pd.read_csv(train_wf).iloc[:,1:]\n",
    "        self.test_df = pd.read_csv(test_index)\n",
    "        self.test_waveforms = pd.read_csv(test_wf).iloc[:,1:]\n",
    "        self.target = target\n",
    "        self.train_subset = None\n",
    "        self.val_subset = None\n",
    "        self.test_subset = None\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_len = fixed_len\n",
    "        self.oversample = oversample\n",
    "        self.center = center\n",
    "        self.xs = [None] * self.df.shape[0]\n",
    "        self.train_ind = None\n",
    "        self.val_ind = None\n",
    "        self.test_ind = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.df[\"Sample\"].iat[idx]\n",
    "        i = self.df[\"original_index\"].iat[idx]\n",
    "        if self.xs[idx] is None:\n",
    "            w = self.waveforms.loc[(self.waveforms[\"Sample\"] == s) & (self.waveforms[\"original_index\"] == i)].iloc[:,:2]\n",
    "            if self.center:\n",
    "                w = w.interpolate().apply(lambda x : (x - np.mean(x)) / np.std(x))\n",
    "            else:\n",
    "                w = w.interpolate().apply(lambda x : x / np.std(x))\n",
    "            if self.fixed_len != None:\n",
    "                if w.shape[0] > self.fixed_len:\n",
    "                    w = w.iloc[:self.fixed_len,:].T\n",
    "                else:\n",
    "                    pad = pd.DataFrame(np.zeros((self.fixed_len - w.shape[0],2)))\n",
    "                    pad.columns = w.columns \n",
    "                    w = pd.concat((w, pad)).T\n",
    "            self.xs[idx] = torch.tensor(w.to_numpy().astype(np.float32))\n",
    "        return torch.tensor(self.df.iloc[idx, :(self.df.shape[1]-3)].to_numpy().astype(np.float32)), self.xs[idx], torch.tensor(self.df[self.target].iat[idx].astype(np.float32))\n",
    "    \n",
    "    def get_train_test_samplers(self, test_size, seed=42):\n",
    "        train, val, _, _ = train_test_split(range(self.df.shape[0]), range(self.df.shape[0]), test_size=test_size, random_state=seed, stratify=self.df[self.target])\n",
    "        if self.oversample:\n",
    "            train_df = self.df.iloc[train, :]\n",
    "            target_n = train_df.loc[train_df[self.target] == 1].shape[0]\n",
    "            off_target_n = train_df.loc[train_df[self.target] == 0].shape[0]\n",
    "            if target_n > off_target_n:\n",
    "                diff = target_n - off_target_n\n",
    "                new_indices = range(self.df.shape[0], self.df.shape[0] + diff)\n",
    "                self.df = pd.concat([self.df, train_df[train_df[self.target] == 0].sample(diff, replace=True)])\n",
    "            else:\n",
    "                diff = off_target_n - target_n\n",
    "                new_indices = range(self.df.shape[0], self.df.shape[0] + diff)\n",
    "                self.df = pd.concat([self.df, train_df[train_df[self.target] == 1].sample(diff, replace=True)])\n",
    "            train = train + list(new_indices)\n",
    "        self.train_ind = train\n",
    "        self.val_ind = val\n",
    "        self.test_ind = list(range(self.df.shape[0], self.df.shape[0] + self.test_df.shape[0]))\n",
    "        self.df = pd.concat([self.df, self.test_df]).reset_index().iloc[:,2:].dropna(axis=1)\n",
    "        self.waveforms = pd.concat([self.waveforms, self.test_waveforms])\n",
    "        self.xs = [None] * self.df.shape[0]\n",
    "        return (SubsetRandomSampler(train), SequentialSampler(val))\n",
    "    \n",
    "    def train_dataloader(self, seed=42):\n",
    "        if self.train_subset is None:\n",
    "            self.train_subset, self.val_subset = self.get_train_test_samplers(0.2, seed=seed)\n",
    "            self.test_subset = SequentialSampler(self.test_ind)\n",
    "        return DataLoader(self, batch_size=self.batch_size, sampler=self.train_subset, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self, seed=42):\n",
    "        if self.val_subset is None:\n",
    "            self.train_subset, self.val_subset = self.get_train_test_samplers(0.2, seed=seed)\n",
    "            self.test_subset = SequentialSampler(self.test_ind)\n",
    "        return DataLoader(self, batch_size=self.batch_size, sampler=self.val_subset, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self, seed=42):\n",
    "        if self.train_subset is None:\n",
    "            self.train_subset, self.val_subset = self.get_train_test_samplers(0.2, seed=seed)\n",
    "            self.test_subset = SequentialSampler(self.test_ind)\n",
    "        return DataLoader(self, batch_size=self.batch_size, sampler=self.test_subset, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395ac402-6749-47f2-a1c5-2082bdae60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_len(lin, pad, dil, ker, stride):\n",
    "    return int((lin + 2 * pad - dil * (ker - 1) - 1) / stride + 1)\n",
    "\n",
    "def pool_len(lin, pad, ker, stride):\n",
    "    return int((lin + 2 * pad - ker) / stride + 1)\n",
    "\n",
    "class AbnormalBreathDetectorCNN(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, k_sizes, dilatations, cout, fixed_len, dropout, pool_kern=None):\n",
    "        super(AbnormalBreathDetectorCNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.k_sizes = k_sizes\n",
    "        self.dilatations = dilatations\n",
    "        self.c_out_n = cout\n",
    "        self.fixed_len = fixed_len\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.pool_kern = pool_kern\n",
    "        self.conv_modules = nn.ModuleList([nn.Conv1d(2, self.c_out_n, k, dilation=d) for d in self.dilatations for k in self.k_sizes])\n",
    "        #self.conv_modules = nn.ModuleList([nn.Conv1d(2, self.c_out_n, k) for k in self.k_sizes])\n",
    "        if self.pool_kern is not None:\n",
    "            self.pooling = nn.AvgPool1d(self.pool_kern)\n",
    "            self.out_sizes = [pool_len(conv_len(self.fixed_len, 0, d, k, 1), 0, self.pool_kern, self.pool_kern) for d in self.dilatations for k in self.k_sizes]\n",
    "        else:\n",
    "            self.out_sizes = [conv_len(self.fixed_len, 0, d, k, 1) for d in self.dilatations for k in self.k_sizes]\n",
    "        self.fc = torch.nn.Linear(sum(self.out_sizes) * self.c_out_n, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.lr = learning_rate\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        if self.pool_kern is not None:\n",
    "            convs = [self.dropout(self.pooling(self.relu(m(xs)))) for m in self.conv_modules]\n",
    "        else:\n",
    "            convs = [self.dropout(self.relu(m(xs))) for m in self.conv_modules]\n",
    "        convs = torch.cat([convs[i].view(-1, self.out_sizes[i] * self.c_out_n) for i in range(len(self.out_sizes))], dim=1)\n",
    "        out = self.fc(convs)\n",
    "        pred = torch.sigmoid(out)\n",
    "        return pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\", patience=5)\n",
    "        return {\"optimizer\" : optimizer, \"lr_scheduler\" : lr_scheduler, \"monitor\" : \"Validation_F1_log\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y)\n",
    "        return {\"loss\" : loss, \"preds\" : y_hat.flatten(), \"labels\" : y}\n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in training_step_outputs]).mean()\n",
    "        preds = torch.cat([x[\"preds\"] for x in training_step_outputs])\n",
    "        targets = torch.cat([x[\"labels\"] for x in training_step_outputs])\n",
    "        print(\"Epoch {} train\".format(self.epoch))\n",
    "        confmat = torchmetrics.ConfusionMatrix(num_classes=2)\n",
    "        print(confmat(preds.cpu(), targets.int().cpu()))\n",
    "        f1score = torchmetrics.F1Score(num_classes=1)\n",
    "        f1_epoch = f1score(preds.cpu(), targets.int().cpu())\n",
    "        recall = torchmetrics.Recall(num_classes=1)\n",
    "        recall_epoch = recall(preds.cpu(), targets.int().cpu())\n",
    "        specificity = torchmetrics.Specificity(num_classes=1)\n",
    "        specificity_epoch = specificity(preds.cpu(), targets.int().cpu())\n",
    "        auroc = torchmetrics.AUROC(pos_label=1)\n",
    "        auroc_epoch = auroc(preds.cpu(), targets.int().cpu())\n",
    "        self.logger.experiment.add_scalar(\"Train_Loss\", avg_loss.item(), self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Train_F1\", f1_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Train_recall\", recall_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Train_specificity\", specificity_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Train_AUROC\", auroc_epoch, self.epoch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y)\n",
    "        return {\"loss\" : loss, \"preds\" : y_hat.flatten(), \"labels\" : y}\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in validation_step_outputs]).mean()\n",
    "        preds = torch.cat([x[\"preds\"] for x in validation_step_outputs])\n",
    "        targets = torch.cat([x[\"labels\"] for x in validation_step_outputs])\n",
    "        print(\"Epoch {} validation\".format(self.epoch))\n",
    "        confmat = torchmetrics.ConfusionMatrix(num_classes=2)\n",
    "        print(confmat(preds.cpu(), targets.int().cpu()))\n",
    "        f1score = torchmetrics.F1Score(num_classes=1)\n",
    "        f1_epoch = f1score(preds.cpu(), targets.int().cpu())\n",
    "        recall = torchmetrics.Recall(num_classes=1)\n",
    "        recall_epoch = recall(preds.cpu(), targets.int().cpu())\n",
    "        specificity = torchmetrics.Specificity(num_classes=1)\n",
    "        specificity_epoch = specificity(preds.cpu(), targets.int().cpu())\n",
    "        auroc = torchmetrics.AUROC(pos_label=1)\n",
    "        auroc_epoch = auroc(preds.cpu(), targets.int().cpu())\n",
    "        self.logger.experiment.add_scalar(\"Validation_Loss\", avg_loss.item(), self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Validation_F1\", f1_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Validation_recall\", recall_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Validation_specificity\", specificity_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Validation_AUROC\", auroc_epoch, self.epoch)\n",
    "        self.log(\"Validation_F1_log\", f1_epoch)\n",
    "        self.epoch += 1\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy(y_hat.flatten(), y)\n",
    "        return {\"loss\" : loss, \"preds\" : y_hat.flatten(), \"labels\" : y}\n",
    "    \n",
    "    def test_epoch_end(self, validation_step_outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in validation_step_outputs]).mean()\n",
    "        preds = torch.cat([x[\"preds\"] for x in validation_step_outputs])\n",
    "        targets = torch.cat([x[\"labels\"] for x in validation_step_outputs])\n",
    "        print(\"Test Results\")\n",
    "        confmat = torchmetrics.ConfusionMatrix(num_classes=2)\n",
    "        print(confmat(preds.cpu(), targets.int().cpu()))\n",
    "        f1score = torchmetrics.F1Score(num_classes=1)\n",
    "        f1_epoch = f1score(preds.cpu(), targets.int().cpu())\n",
    "        recall = torchmetrics.Recall(num_classes=1)\n",
    "        recall_epoch = recall(preds.cpu(), targets.int().cpu())\n",
    "        specificity = torchmetrics.Specificity(num_classes=1)\n",
    "        specificity_epoch = specificity(preds.cpu(), targets.int().cpu())\n",
    "        auroc = torchmetrics.AUROC(pos_label=1)\n",
    "        auroc_epoch = auroc(preds.cpu(), targets.int().cpu())\n",
    "        self.logger.experiment.add_scalar(\"Test_Loss\", avg_loss.item(), self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Test_F1\", f1_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Test_Recall\", recall_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Test_Specificity\", specificity_epoch, self.epoch)\n",
    "        self.logger.experiment.add_scalar(\"Test_AUROC\", auroc_epoch, self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "breath_max_len = 300\n",
    "batch_size = 64\n",
    "max_epoch = 100\n",
    "oversampling = True\n",
    "dropout = 0\n",
    "seed = 641\n",
    "asynchronies = [\"early trigger\", \"late trigger\", \"failed trigger\", \"multiple trigger\", \"early cycling\", \"late cycling\", \"expiratory work\", \"splinting\"]\n",
    "kernal_sizes = [5,11,21]\n",
    "dilatation_sizes = [1]\n",
    "pooling = [False]\n",
    "lrs = [0.001]\n",
    "\n",
    "for l in lrs:\n",
    "    for p in pooling:\n",
    "        for a in asynchronies:\n",
    "            data = NeonatalVentilationDataset(\"../data/train/combined.csv\", \"../data/train/waveforms.csv\",\n",
    "                                              \"../data/test/combined.csv\", \"../data/test/waveforms.csv\",\n",
    "                                              a, batch_size, breath_max_len, oversampling, center=True)\n",
    "            stem = \"CNN_lr{}_kern{}_dil{}_filters128_{}_{}\".format(l, \"-\".join([str(x) for x in kernal_sizes]), \"-\".join([str(x) for x in dilatation_sizes]), \"pooled\" if p else \"no_pool\", a)\n",
    "            if os.path.isdir(stem):\n",
    "                continue\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), version=stem)\n",
    "            checkpoint_cb = ModelCheckpoint(save_top_k = 1, \n",
    "                                            monitor=\"Validation_F1_log\", \n",
    "                                            mode=\"max\", \n",
    "                                            dirpath=\"model_checkpoints\",\n",
    "                                            filename=stem + \"{epoch:02d}-{Validation_F1_log:.2f}\")\n",
    "            earlystopping_cb = EarlyStopping(monitor=\"Validation_F1_log\", mode=\"max\", patience=10)\n",
    "            if p:\n",
    "                model = AbnormalBreathDetectorCNN(l, kernal_sizes, dilatation_sizes, 128, breath_max_len, dropout, pool_kern = 3)\n",
    "            else:\n",
    "                model = AbnormalBreathDetectorCNN(l, kernal_sizes, dilatation_sizes, 128, breath_max_len, dropout)\n",
    "            trainer = pl.Trainer(track_grad_norm=2, max_epochs=max_epoch, accelerator=\"gpu\", devices=1, \n",
    "                                 callbacks=[earlystopping_cb, checkpoint_cb], logger=tb_logger)\n",
    "            trainer.fit(model=model, train_dataloaders=data.train_dataloader(seed), val_dataloaders=data.val_dataloader(seed))\n",
    "            trainer.test(model=model, dataloaders=data.test_dataloader(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260b054f-1a69-466f-a4ca-295c8d791d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.6.0 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\David Chong\\Desktop\\backup\\Ventilation_Asynchrony_Classifier_Paper\\model_dev\\model_checkpoints\\CNN_lr0.001_kern5-11-21_dil1_filters128_no_pool_failed triggerepoch=37-Validation_F1_log=0.91.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2364,   11],\n",
      "        [  10,  115]])\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AbnormalBreathDetectorCNN.load_from_checkpoint(\"model_checkpoints/CNN_lr0.001_kern5-11-21_dil1_filters128_no_pool_failed triggerepoch=37-Validation_F1_log=0.91.ckpt\") \n",
    "#                                                       learning_rate = 0.001, k_sizes = [5,11,21], dilatations = [1], cout = 128, fixed_len = 300, dropout = 0.5)\n",
    "data = NeonatalVentilationDataset(\"../data/train/combined.csv\", \"../data/train/waveforms.csv\",\n",
    "                                              \"../data/test/combined.csv\", \"../data/test/waveforms.csv\",\n",
    "                                              \"failed trigger\", 64, 300, True, center=True)\n",
    "i = 0\n",
    "y_hat = []\n",
    "ys = []\n",
    "val_set = iter(data.test_dataloader(641))\n",
    "_, x, y = next(val_set)\n",
    "while x is not None:\n",
    "    y_hat.append(loaded_model(x.cuda()).flatten())\n",
    "    ys.append(y.flatten())\n",
    "    _, x, y = next(val_set, (None, None, None))\n",
    "y_hat = torch.cat(y_hat).cpu()\n",
    "ys = torch.cat(ys).cpu()\n",
    "confmat = torchmetrics.ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "print(confmat(y_hat, ys.int()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c459966-8d36-4345-b442-08eb7599b139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "early trigger        2\n",
       "late trigger         0\n",
       "failed trigger       3\n",
       "multiple trigger     0\n",
       "early cycling        0\n",
       "late cycling         0\n",
       "expiratory work      6\n",
       "splinting            0\n",
       "work shifting        0\n",
       "artefact             0\n",
       "unassisted breath    1\n",
       "no asynchrony        3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg = (ys == 1) & (torch.round(y_hat) == 0)\n",
    "false_pos = (ys == 0) & (torch.round(y_hat) == 1)\n",
    "asynchronies = [\"early trigger\", \"late trigger\", \"failed trigger\", \"multiple trigger\", \"early cycling\", \"late cycling\", \"expiratory work\", \"splinting\", \"work shifting\", \"artefact\", \"unassisted breath\", \"no asynchrony\"]\n",
    "data.df.iloc[data.test_ind,:].reset_index().loc[false_pos.numpy()][asynchronies].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8e3acd-2a4d-4b4e-8713-ce44f4b746b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "early trigger        0\n",
       "late trigger         0\n",
       "failed trigger       0\n",
       "multiple trigger     1\n",
       "early cycling        0\n",
       "late cycling         3\n",
       "expiratory work      6\n",
       "splinting            0\n",
       "work shifting        0\n",
       "artefact             1\n",
       "unassisted breath    0\n",
       "no asynchrony        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df.iloc[data.test_ind,:].reset_index().loc[false_neg.numpy()][asynchronies].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c17d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
